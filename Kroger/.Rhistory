#Report RMSE on holdout sample
postResample( predict(GBM,newdata=HOLDOUT), HOLDOUT$TipPercentage)
#Report variable importance plot
varImp(GBM)
#The most important variable again is Bill. The second most important is SmokerYes. There are a lot of similarities among all of these models.
library(neuralnet)
set.seed(474); nnetGrid <- expand.grid(size=c(1:6),
decay=10^seq(-2,1,0.5))
set.seed(474); NNET <- train(TipPercentage~., data=TRAIN, method="nnet",
trControl=fitControl,
verbose=FALSE,
trace=FALSE,
linout=TRUE,
tuneGrid=nnetGrid,
preProc = c("center", "scale"))
#Report estimated generalization RMSE
NNET$results[rownames(NNET$bestTune), ]
#Report RMSE on the holdout sample
postResample(predict(NNET, newdata=HOLDOUT), HOLDOUT$TipPercentage)["RMSE"]
#Report the variable importance plot
varImp(NNET)
#The most important predictor in predicting TipPercentage is Bill. A second important predictor is SmokerYes.
RMSEVector <- c()
#GLM RMSE
RMSEVector["GLM"] <- MODEL$results[["RMSE"]]
#GLMNet RMSE
RMSEVector["GLMNet"] <- GLMnet$results[rownames(GLMnet$bestTune),][["RMSE"]]
#Regression Tree model
RMSEVector["RPART"] <- RPART$results[rownames(RPART$bestTune),][["RMSE"]]
#RF RMSE
RMSEVector["FOREST"] <- FOREST$results[rownames(FOREST$bestTune), ][["RMSE"]]
#GBM RMSE
RMSEVector["GBM"] <- GBM$results[rownames(GBM$bestTune),][["RMSE"]]
#NNET RMSE
RMSEVector["NNET"] <- NNET$results[rownames(NNET$bestTune), ][["RMSE"]]
#barplot to compare model RMSEs
RMSEVector
barplot(RMSEVector,)
#The model with the lowest RMSE is the gradient boosted tree model by a little bit. However, its RMSESD is very big at 2.038. That means all the other models are within one SD of the model with the lowest RMSE. Therefore, one model is not a compelling choice for predicting TipPercentage over others.
PURCHASE <- read.csv("PURCHASE.csv")
set.seed(474); train.rows <- sample(nrow(PURCHASE),1000)
PURCHASE_TRAIN <- PURCHASE[train.rows,];
PURCHASE_HOLDOUT <- PURCHASE[-train.rows,]
# set fitControl for estimating the generalization error. Here use AUC to train and compare models.
fitControl <- trainControl(method="cv",number=5,summaryFunction=twoClassSummary,classProbs=TRUE)
#find which class is the majority in the training data
table(PURCHASE_TRAIN$Purchase)
#No is the majority so the naive model will classify everyone in the data to No
#find estimated generalization accuracy of the model
mean(PURCHASE_TRAIN$Purchase == "No")
#find the accuracy on the holdout sample
mean( PURCHASE_HOLDOUT$Purchase == "No" )
GLMModel <- train(Purchase~.,data=PURCHASE_TRAIN,
method="glm",
trControl=fitControl,
preProc=c("center", "scale") )
#Report the estimated generalization metrics of the best model
GLMModel$results
#Report the accuracy and the auc on the holdout sample
#Accuracy
mean( PURCHASE_HOLDOUT$Purchase == predict(GLMModel,newdata=PURCHASE_HOLDOUT) )
#AUC
roc(PURCHASE_HOLDOUT$Purchase,predict(GLMModel,newdata=PURCHASE_HOLDOUT,type="prob")[,2])
#Report the variable importance plot
varImp(GLMModel)
#The predictor that is most important for predicting Purchase is Visits by far. The second highest predictor is Closest.
glmnetGrid <- expand.grid(alpha = seq(0,1,0.1),lambda = 10^seq(-3,2,by=.5))
set.seed(474); GLMnet <- train(Purchase~.,data=PURCHASE_TRAIN,method='glmnet', tuneGrid=glmnetGrid,
trControl=fitControl, preProc = c("center", "scale"))
#Report the estimated generalization metrics for the best model
GLMnet$results[rownames(GLMnet$bestTune),]
#Report accuracy and auc on the holdout samples
#Accuracy
mean( PURCHASE_HOLDOUT$Purchase == predict(GLMnet,newdata=PURCHASE_HOLDOUT) )
#AUC
roc(PURCHASE_HOLDOUT$Purchase,predict(GLMnet,newdata=PURCHASE_HOLDOUT,type="prob")[,2])
#Report the variable importance plot
varImp(GLMnet)
#The most important predictor is Visits in predicting Purchase. There appears to be no other predictors that help in predicting Purchase.
rpartGrid <- expand.grid(cp=10^seq(-5,-1,length=50))
set.seed(474); RPART <- train(Purchase~.,data=PURCHASE_TRAIN,method="rpart",trControl=fitControl,
tuneGrid=rpartGrid,preProc=c("center","scale"))
#Report the estimated generalization metrics of the best model
RPART$results[rownames(RPART$bestTune),]
#Report accuracy and auc on the holdout sample
#Accuracy
mean( PURCHASE_HOLDOUT$Purchase == predict(RPART,newdata=PURCHASE_HOLDOUT) )
# AUC
roc(PURCHASE_HOLDOUT$Purchase,predict(RPART,newdata=PURCHASE_HOLDOUT,type="prob")[,2])
#Report variable importance plot
varImp(RPART)
#The most important predictor is Spent which different from the above models. In second is Closest. In third is PercentClose. Finally in fourth, we have Visits, which is the top predictor in the above models.
forestGrid <- expand.grid(mtry=c(1,5))
set.seed(474); FOREST <- train(Purchase~.,data=PURCHASE_TRAIN,method="rf",preProc=c("center","scale"),
trControl=fitControl,tuneGrid=forestGrid, importances=TRUE)
#Report the estimated generalization metrics of the best model
FOREST$results[rownames(FOREST$bestTune),]
#Report accuracy and auc on the holdout sample
#Accuracy
mean( PURCHASE_HOLDOUT$Purchase == predict(FOREST,newdata=PURCHASE_HOLDOUT) )
#AUC
roc(PURCHASE_HOLDOUT$Purchase,predict(FOREST,newdata=PURCHASE_HOLDOUT,type="prob")[,2])
#Report variable importance plot
varImp(FOREST)
#The most important variable for predicting Purchase in this model is Spent. In second, there is PercentClose. In third, there is Closest. In fourth, there is Visits, which is the top predictor in some of the other models.
gbmGrid <- expand.grid(n.trees=c(200,500),
interaction.depth=1:3,
shrinkage=c(.05,.1),
n.minobsinnode=c(2,5,10))
set.seed(474); GBM <- train(Purchase~.,data=PURCHASE_TRAIN,method="gbm",trControl=fitControl,
tuneGrid=gbmGrid,preProc=c("center","scale"),verbose=FALSE)
#Report the estimated generalization metrics of the best model
GBM$results[rownames(GBM$bestTune),]
#Report accuracy and auc on the holdout sample
#Accuracy
mean( PURCHASE_HOLDOUT$Purchase == predict(GBM,newdata=PURCHASE_HOLDOUT) )
#AUC
roc(PURCHASE_HOLDOUT$Purchase,predict(GBM,newdata=PURCHASE_HOLDOUT,type="prob")[,2])
#Report variable importance plot
varImp(GBM)
#The most important Predictor is Spent. In second, there is PercentClose. In third, there is Visits.
nnetGrid <- expand.grid(size=c(1:6),decay=10^( seq(-2,1,by=.5) ) )
set.seed(474); NNET <- train(Purchase~.,
data=PURCHASE_TRAIN,
method="nnet",
trControl=fitControl,
tuneGrid=nnetGrid,
trace=FALSE,
verbose=FALSE,
linout=FALSE,
preProc = c("center", "scale"))
#Report the estimated generalization metrics of the best model
NNET$results[rownames(NNET$bestTune),]
#Report the accuracy and auc on the holdout sample
#Accuracy
mean( PURCHASE_HOLDOUT$Purchase == predict(NNET,newdata=PURCHASE_HOLDOUT) )
#AUC
roc(PURCHASE_HOLDOUT$Purchase,predict(NNET,newdata=PURCHASE_HOLDOUT,type="prob")[,2])
#Report variable importance plot
varImp(NNET)
# The most important predictor for Purchase in this model is Visits. This is different than some of the above models. In second, there is CloseStores. This is a new top predictor. In third, there is Spent.
ModelComparison <- c()
#Generalization Error of logistic regression model
ModelComparison["GLM"] <-  GLMModel$results[["ROC"]]
#Generalization Error of regularized logistic regression
ModelComparison["GLMNet"] <- GLMnet$results[rownames(GLMnet$bestTune),][["ROC"]]
#Generalization Error of classification tree model
ModelComparison["RPART"] <-  RPART$results[rownames(RPART$bestTune),][["ROC"]]
#Generalization Error of random forest
ModelComparison["FOREST"] <- FOREST$results[rownames(FOREST$bestTune),][["ROC"]]
#Generalization Error of gradient boosted tree
ModelComparison["GBM"] <-  GBM$results[rownames(GBM$bestTune),][["ROC"]]
#Generalization Error of neural net
ModelComparison["NNET"] <- NNET$results[rownames(NNET$bestTune),][["ROC"]]
#Visualization
ModelComparison
barplot(ModelComparison)
#find estimated generalization accuracy of the model
mean(PURCHASE_TRAIN$Purchase == "No")
#Report the estimated generalization metrics of the best model
RPART$results[rownames(RPART$bestTune),]
0.03899638 + 0.5661582
GLMModel$results
#Visualization
ModelComparison
FOREST$results[rownames(FOREST$bestTune),]
#Visualization
ModelComparison
0.6364147 - 0.06806732
#find estimated generalization accuracy of the model
mean(PURCHASE_TRAIN$Purchase == "No")
#find the accuracy on the holdout sample
mean( PURCHASE_HOLDOUT$Purchase == "No" )
CA <- read.csv("CAvideos.csv", stringsAsFactors = FALSE)
JA <- read.csv("JPvideos.csv", stringsAsFactors = FALSE)
MX <- read.csv("MXvideos.csv", stringsAsFactors = FALSE)
MX <- read.csv("MXvideos.csv", stringsAsFactors = FALSE)
US <- read.csv("USvideos.csv", stringsAsFactors = FALSE)
MX <- read.csv("MXvideos.csv", stringsAsFactors = FALSE)
US <- read.csv("USvideos.csv", stringsAsFactors = FALSE)
CA$Country <- rep("CA", nrow(CA))
JA$Country <- rep("JA", nrow(JA))
MX$Country <- rep("MX", nrow(MX))
US$Country <- rep("US", nrow(US))
YOUTUBE <- rbind(CA,JA,MX,US)
table(YOUTUBE)
table(YOUTUBE$Country)
YOUTUBE$Country <- factor(YOUTUBE$Country)
YOUTUBE$LikesToView <- YOUTUBE$likes / YOUTUBE$views
YOUTUBE$Compelled <- factor(ifelse(YOUTUBE$LikesToView >= 0.1,"Yes","No"))
mean(YOUTUBE$LikesToView)
IDs <- read.csv('category_id.csv', stringsAsFactors = TRUE)
FLAT <- merge(YOUTUBE, IDs,by.x = "category_id", by.y = "CategoryID",all.x = TRUE)
summary(FLAT[,c("category_id","LikesToView","Compelled","Country","Category")])
mean(FLAT$views)
median(FLAT$LikesToView)
P.V.Country <- aggregate(Compelled =='Yes' ~ Country, data=FLAT, FUN=mean )
P.V.Cat <- aggregate(Compelled=='Yes' ~ Category, data=FLAT,FUN=mean)
P.V.CountryCat <- aggregate(Compelled =='Yes' ~ Country + Category, data=FLAT, FUN=mean, drop=FALSE)
n.V.CountryCat <- aggregate(title ~ Country + Category, data=FLAT, FUN = length, drop=FALSE)
names(n.V.CountryCat) <- c("Country", "Category", "n")
mean(P.V.Cat[,2])
SUMMARY <- merge(P.V.CountryCat, n.V.CountryCat, by=c("Country", "Category"))
names(SUMMARY)[3] <- "ProbCompelled"
mean(SUMMARY$n,na.rm=TRUE)
head(SUMMARY)
SUMMARY$n >3000
which(SUMMARY$n >3000)
SUMMARY[which(SUMMARY$n >3000),]
SUMMARY <- SUMMARY[which(SUMMARY$n >3000),]
SUMMARY <- merge(P.V.CountryCat, n.V.CountryCat, by=c("Country", "Category"))
names(SUMMARY)[3] <- "ProbCompelled"
S2 <- subset(SUMMARY,n>=3000)
head(S2[order(S2$ProbCompelled,decreasing = TRUE),])
head(S2[order(S2$ProbCompelled,decreasing = TRUE),],7)
integers <- sample(20:50,size=8,replace=TRUE)
integers
(max(integers) - min(integers))
((max(integers) - min(integers)) >= 25)
ntrials <- 100000
counter <- 0
for (i in 1:ntrials) {
integers <- sample(20:50,size=8,replace=TRUE)
if ((max(integers) - min(integers)) >= 25) {counter <- counter + 1}
}
counter / ntrials
x <- c(3,5,8,10)
cannon <- 0
for ( i in 1:3 ) {
cannon <- cannon + 2*x[i]
}
cannon
win.A <- 0
win.B <- 0
tie <- 0
ntrials <- 100000
#Simulation
for (i in 1:ntrials) {
buzz.A <- sample(c(0,1),size = 30, replace = TRUE, prob = c(.6,.4))
buzz.B <- 1 - buzz.A
correct.A <- sample(c(0,1),size = 30, replace = TRUE, prob = c(.2,.8))
correct.B <- 1 - correct.A
score.A <- sum(buzz.A * correct.A)
score.B <- sum(buzz.B * correct.B)
if (score.A > score.B) {win.A <- win.A + 1} #A wins
else if (score.B > score.A) {win.B <- win.B + 1} #B wins
else {tie <- tie + 1} #they are tied
}
win.A/100000
win.B/100000
tie/100000
win.A <- 0
win.B <- 0
tie <- 0
ntrials <- 100000
#Simulation
for (i in 1:ntrials) {
buzz.A <- sample(c(0,1),size = 30, replace = TRUE, prob = c(.6,.4))
buzz.B <- 1 - buzz.A
correct.A <- sample(c(0,1),size = 30, replace = TRUE, prob = c(.2,.8))
correct.B <- 1 - correct.A
score.A <- sum(buzz.A * correct.A)
score.B <- sum(buzz.B * correct.B)
if (score.A > score.B) {win.A <- win.A + 1} #A wins
else if (score.B > score.A) {win.B <- win.B + 1} #B wins
else {tie <- tie + 1} #they are tied
}
win.A/100000
win.B/100000
tie/100000
#Simulation
buzz.A <- sample(c(0,1),size = 30, replace = TRUE,prob = c(0.6,0.4) )
buzz.A
mean(buzz.A)
#Simulation
buzz.A <- sample(c(0,1),size = 30, replace = TRUE,prob = c(0.6,0.4) )
mean(buzz.A)
#Simulation
buzz.A <- sample(c(0,1),size = 30, replace = TRUE,prob = c(0.6,0.4) )
mean(buzz.A)
#Simulation
buzz.A <- sample(c(0,1),size = 30, replace = TRUE,prob = c(0.6,0.4) )
mean(buzz.A)
#Simulation
buzz.A <- sample(c(0,1),size = 30, replace = TRUE,prob = c(0.6,0.4) )
mean(buzz.A)
buzz.B <- 1 - buzz.A
buzz.A
buzz.B
correct.A <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.2,0.8))
mean(correct.A)
correct.A <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.2,0.8))
mean(correct.A)
correct.A <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.2,0.8))
mean(correct.A)
correct.A <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.2,0.8))
mean(correct.A)
correct.A <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.2,0.8))
mean(correct.A)
correct.B <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.5,0.5))
mean(correct.B)
correct.B <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.5,0.5))
mean(correct.B)
score.A <- sum(buzz.A * correct.A)
buzz.A * correct.A
buzz.A
correct.A
score.B <- sum(buzz.B * correct.B)
?if
{}
win.A <- 0
win.B <- 0
tie <- 0
ntrials <- 100000
#Simulation
for (i in 1:ntrials) {
buzz.A <- sample(c(0,1),size = 30, replace = TRUE,prob = c(0.6,0.4) )
buzz.B <- 1 - buzz.A
correct.A <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.2,0.8))
correct.B <- sample(c(0,1), size = 30, replace = TRUE, prob = c(0.5,0.5))
score.A <- sum(buzz.A * correct.A)
score.B <- sum(buzz.B * correct.B)
if (score.A > score.B) {
win.A <- win.A + 1
} else if (score.B > score.A) {
win.B <- win.B + 1
} else if (score.A == score.B) {
tie <- tie + 1
} else {
print("there is a problem")
}
}
win.A/100000
win.B/100000
tie/100000
ntrials <- 100000
wins <- c()
for (i in 1:ntrials) {
games <- sum(sample( c(0,1), size=7, replace=TRUE, prob=c(0.3,0.7)))
if (games >= 4) { wins <- c(wins, "B")} else {wins <- c(wins, "A")}
}
mean(wins == "B")
prop.test(c(20,150),c(150,850), conf.level = .95)
library(regclass)
data("CUSTLOYALTY")
head(CUSTLOYALTY)
aggregate(LoyaltyCard == "Yes" ~ Income, data = CUSTLOYALTY)
aggregate(LoyaltyCard == "Yes" ~ Income, data = CUSTLOYALTY, FUN=mean)
install.packages("shiny")
x <- 1:10
shape <- x*(12-x)
barplot(shape,names.arg=x)
barplot(shape,names.arg=x)
sum(which(x==5))
shape[sum(which(x==5))]
shape
p <- shape / sum(shape)
sum(p)
p[sum(which(x==5))]
x <- 2:60; p <- c(.75,.5^(x[-1]))
sum(p)  #valid set of probabilities
p[sum(which(x <= 7))]
x <- 2:60; p <- c(.75,.5^(x[-1]))
sum(p)  #valid set of probabilities
p[sum(which(x <= 7))]
x <- 2:60; p <- c(.75,.5^(x[-1]))
sum(p)  #valid set of probabilities!
p
sum(which(p <= 7))
sum(which(p <= 7)) / sum(p)
sum(which(p <= 7)) / sum(p)
x
p >=<= 7
p <= 7
which(x <= 7)
sum(which(x <= 7))
x <- 2:60; p <- c(.75,.5^(x[-1]))
sum(p)  #valid set of probabilities!
sum(which(x <= 7))
p[which(x <= 7)]
sum(p[which(x <= 7)])
#INSERT WORK
#Given search along trail C was unsuccessful...
HikerAlongA <- 400 / 725
#INSERT WORK
#Given search along trail C was unsuccessful...
HikerAlongA <- 400 / 725
HikerAlongB <- 100 / 725
HikerAlongC <- 225 / 725
HikerAlongA
HikerAlongB
HikerAlongC
#INSERT WORK
#Given that the search along trails A and C were unsuccessful...
HikerAlongA <- 160 / 485
HikerAlongB <- 100 / 485
HikerAlongC <- 225 / 485
HikerAlongA
HikerAlongB
HikerAlongC
trail.record <- c()  #a vector to keep track of where hiker actually is
search.record1 <- c()  #a vector to keep track of whether the 1st search on C is successful (successful vs. unsuccessful)
search.record2 <- c()  #a vector to keep track of whether the 2nd search on C is successful (none vs. successful vs. unsuccessful)
for (i in 1:1e5) {
trail <- sample( c("A","B","C"),1,prob=c(.4,.1,.5) ) #actual location of hiker
#determine if search on C is successful or not
if( trail %in% c("A","B") ) { search.on.C <- "unsuccessful" } else {
search.on.C <- sample( c("unsuccessful","successful"), size=1, prob=c(.45,.55) )
}
trail.record[i] <- trail
search.record1[i] <- search.on.C
search.record2[i] <- "none"  #start out with saying no search on A, then figure out if one takes place and its outcome
#if the search on C was unsuccessful, determine if the search on A is unsuccessful
if(search.on.C=="unsuccessful") {
if( trail %in% c("C","B") ) {
search.on.A <- "unsuccessful" } else {
search.on.A <- sample( c("unsuccessful","successful"), size=1, prob=c(.4,.6) ) }
search.record2[i] <- search.on.A
}
}
#Make dataframe of where hiker was and outcomes of searches on C and A
DATA <- data.frame(trail=trail.record,searchC=search.record1,searchA=search.record2,stringsAsFactors = TRUE)
summary(DATA)
#Reduce the sample space by including only rows where searches on A and C are unsuccessful
CONDITION <- subset(DATA,searchC =="unsuccessful" & searchA == "unsuccessful" )
#Estimated condition probabilities (and your sanity check for part b)
table(CONDITION$trail)/nrow(CONDITION)
HikerAlongA
HikerAlongB
HikerAlongC
#INSERT WORK
#P(A on computer | user on amazon)...
p <- 120 / 360
p
GoldCoinP <- 40/100
BronzeCoinP <- 40/99
GoldCoinP * BronzeCoinP
GoldCoinP <- 20/100
BronzeCoinP <- 10/99
GoldCoinP * BronzeCoinP
GoldCoinP <- 10/100
BronzeCoinP <- 70/99
GoldCoinP * BronzeCoinP
(0.1616162 + 0.07070707 + .02020202) * (1/3)
(0.1616162*(1/3))/(0.0841751)
48/75
shiny::runApp('C:/Users/alexd/Desktop/schoolWork/utk/utkspring2021/bas479/case3/Kroger')
#load in the data
load('KrogerHouseholdsCarbs.RData')
load('KrogerShiny.RData')
setwd("C:/Users/alexd/Desktop/schoolWork/utk/utkspring2021/bas479/case3/Kroger")
#load in the data
load('KrogerHouseholdsCarbs.RData')
load('KrogerShiny.RData')
Data <- subset(HOUSE, City == "Knoxville")
Data
Data <- Data[,c("PancakeMoney", "PastaMoney", "SauceMoney", "SyrupMoney")]
Data <- c(mean(Data$PancakeMoney), mean(Data$PastaMoney), mean(Data$SauceMoney), mean(Data$SyrupMoney))
Data
runApp()
runApp()
Data <- subset(HOUSE, City == "Knoxville")
Data
#ggplot(Data, aes(x=item,y=value)) +
#    geom_bar(stat = "identity") +
#    ggtitle("Average Revenue from different types of products")
Data2 <- c(sum(Data$NumCoupons), sum(Data$NumVisits), sum(Data$NumberOfStores))
Data2
runApp()
levels(HOUSE$MostCommonCommodity)
levels(HOUSE$MostCommonProd)
runApp()
Data
Data <- subset(HOUSE, City == "Knoxville")
Data <- Data[,c("PancakeMoney", "PastaMoney", "SauceMoney", "SyrupMoney")]
Data <- c(mean(Data$PancakeMoney), mean(Data$PastaMoney), mean(Data$SauceMoney), mean(Data$SyrupMoney))
Data
round(Data)
round(Data, digits=2)
MONEY
runApp()
runApp()
"hee" + "dd"
paste("hh", "hh")
runApp()
runApp()
runApp()
install.packages('rsconnect')
HOUSE$household
head(HOUSE)
runApp()
runApp()
runApp()
runApp()
c("NumCoupons", "NumVisits", "NumberofStores")
Data2 <- subset(HOUSE,StateID == input$state)
Data2 <- subset(HOUSE,StateID == "TN")
runApp()
runApp()
data <- c()
if("NumCoupons" %in% input$stat) {data <- c(data,sum(Data2$NumCoupons)) }
Data2
runApp()
HOUSE$MostCommonProd
frequency(HOUSE$MostCommonProd)
unique(HOUSE$MostCommonCommodity)
unique(HOUSE$MostCommonProd)
unique(c("h","h"))
merge(HOUSE,KROGER,by=H)
?merge
merge(HOUSE,KROGER,by.x = household,by.y = HouseHoldID)
merge(HOUSE,KROGER,by.x = "household",by.y = "HouseHoldID")
